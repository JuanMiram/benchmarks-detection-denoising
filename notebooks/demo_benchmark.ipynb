{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A benchmark for time-frequency denoising/ detection methods\n",
    "\n",
    "## What is this benchmark?\n",
    "A benchmark is a comparison between different methods when running an standardized test. The goal of this benchmark is to compare different methods for denoising / detecting a signal based on different characterizations of the of the time-frequency representation of the signal. This particular benchmark has been created for evaluating the performance of techniques based on the zeros of the spectrogram and to contrast them with more traditional methods, like those based on the ridges of that time-frequency distribution.\n",
    "\n",
    "Nevertheless, with the purpose of making this benchmark more useful, the methods to compare, the tests, and the performance evaluation functions were conceived as different modules, so that one can assess new methods without modifing the tests. The only restriction this pose is that the methods should satisfy some requirements regarding the shape of their input an output parameters.  On the one hand, the tests and the performance evaluation functions, are encapsulated in the class `Benchmark`. On the other hand, the signals used in this benchmark are generated by the methods in the class `SignalBank`.\n",
    "\n",
    "In order to compare different methods with possibly different parameters, we need to set up a few things before running the benchmark. A `Benchmark` object receives some input parameters to configure the test:\n",
    "- `task`: This could be `'denoising'` or `'detection'`. The first one compute the quality reconstruction factor (QRF) using the output of the method, whereas the second simply consist in detecting whether a signal is present or not.\n",
    "- `N`: The length of the simulation, i.e. how many samples should the signals have.\n",
    "- `methods`: A dictionary of methods. Each entry of this dictionary corresponds to the function that implements each of the desired methods.\n",
    "- `parameters`: A dictionary of parameters. Each entry of this dictionary corresponds to an array of tuples to pass as input parameters to each method. In order to know which parameters should be passed to each methods, the keys of this dictionary should be the same as those corresponding to the individual methods in the corresponding dictionary. An example of this is showed below.\n",
    "- `SNRin`: A list or tuple of values of SNR to test.\n",
    "- `repetitions`: The number of times the experiment should be repeated with different realizations of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A dummy test \n",
    "First let us define a dummy method for testing. Methods should receive a numpy array with shape  ```(N,)``` where `N` is the number of time samples of the signal. Additionally, they can receive any number of positional or keyword arguments to allow testing different combinations of input parameters. The shape of the output depends on the task (signal denoising or detection). So the recommended signature of a method should be the following:\n",
    "\n",
    " `output = a_method(noisy_signal, *args, **kwargs) `.\n",
    "\n",
    "If one set `task='denoising'`, `output` shoud be a `(N,)` numpy array, i.e. the same shape as the input parameter `noisy_signal`, whereas if `task='detection'`, the output should be boolean (`0` or `False` for no signal, and `1` or `True` otherwise).\n",
    "\n",
    "After this, we need to create a *dictionary of methods* to pass the `Benchmark` object at the moment of instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi as pi\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from benchmark_demo.Benchmark import Benchmark\n",
    "from benchmark_demo.ResultsInterpreter import ResultsInterpreter\n",
    "\n",
    "def a_method(noisy_signal, *args, **kwargs):\n",
    "    # If additional input parameters are needed, they can be passed in a tuple using \n",
    "    # *args or **kwargs and then parsed.\n",
    "    results = noisy_signal # Simply return the same noisy signals.\n",
    "    return results\n",
    "\n",
    "def another_method(noisy_signal, *args, **kwargs):\n",
    "    # If additional input parameters are needed, they can be passed in a tuple using `params` and then parsed.\n",
    "    results = 2*noisy_signal # Simply return the same noisy signals.\n",
    "    return results\n",
    "\n",
    "# Create a dictionary of the methods to test.\n",
    "my_methods = {\n",
    "    'Method 1': a_method, \n",
    "    'Method 2': another_method,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `params` in the example above allows us to pass some parameters to our method. This would be useful for testing a single method with several combination of input parameters. In order to do this, we should give the `Benchmark` object a *dictionary of parameters*. An example of this functionality is showed in the next section. For now, lets set the input parameter `parameters = None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to instantiate a `Benchmark` object and run a test using the proposed methods and parameters. The benchmark constructor receives a name of a task (which defines the performance function of the test), a dictionary of the methods to test, the desired length of the signals used in the simulation, a dictionary of different parameters that should be passed to the methods, an array with different values of SNR to test, and the number of repetitions that should be used for each test. Once the object is created, use the class method `run_test()` to start the experiments.\n",
    "\n",
    "*Remark 1: You can use the ```verbosity``` parameter to show less or more messages during the progress of the experiments. There are 4 levels of verbosity, from ```verbosity=0``` (indicate just the start and the end of the experiments) to ```verbostiy = 4``` (show each method and parameter progress)*\n",
    "\n",
    "*Remark 2: Parallelize the experiments is also possible by passing the parameter ```parallelize = True```. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = Benchmark(task = 'denoising',\n",
    "                        methods = my_methods,\n",
    "                        N = 256, \n",
    "                        SNRin = [40, 50], \n",
    "                        repetitions = 3,\n",
    "                        using_signals=['LinearChirp', 'CosChirp',],\n",
    "                        verbosity=4, \n",
    "                        parallelize=False)\n",
    "                        \n",
    "benchmark.run_test() # Run the test. my_results is a dictionary with the results for each of the variables of the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the results of the test in a nested dictionary called `my_results`. In order to get the results in a human-readable way using a `DataFrame`, and also for further analysis and reproducibility, we can use the class method `get_results_as_df()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = benchmark.get_results_as_df() # This formats the results on a DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the DataFrame show the results ordered by columns. The first column corresponds to the method identification, and the values are taken from the keys of the dictionary of methods. The second column enumerates the parameters used (more on this on the next section). The third column corresponds to the signal identification, using the signal identification values from the `SignalBank` class. The next column shows the number of repetition of the experiment. Finally, the remaining columns show the results obtained for the SNR values used for each experiment. Since `task = 'denoising'`, these values correspond to the QRF computed as `QRF = 10*np.log10(E(s)/E(s-sr))`, where `E(x)` is the energy of `x`, and `s` and `sr` are the noiseless signal and the reconstructed signal respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing different parameters to the methods.\n",
    "It is common that a method depends on certain input parameters (thresholds, multiplicative factors, etc). Therefore, it would be useful that the tests could also be repeated with different parameters, instead of creating multiple versions of one method. We can pass an array of parameters to a method provided it parses them internally. In order to indicate the benchmark which parameters combinations should be given to each method, a dictionary of parameters can be given. \n",
    "\n",
    "Let us now create this dictionary. The parameters combinations should be given in a tuple of tuples, so that each internal tuple is passed as the additional parameter (the corresponding method, of course, should implement how to deal with the variable number of input parameters). For this to work, **the keys of this dictionary should be the same as those of the methods dictionary**. \n",
    "\n",
    "We can now see more in detail how to pass different parameters to our methods. For instance, let's consider a function that depends on two thresholds `thr1` (a positional argument) and `thr2` (a keyword argument):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_function(signal, thr1, thr2=0):\n",
    "# def a_function(signal, thr1, thr2):\n",
    "    print(thr1,thr2)    \n",
    "    signal_out = signal\n",
    "    signal_out[np.where((thr2>signal) & (signal>thr1))] = 0\n",
    "    return signal_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a method that wraps the previous function and then define the dictionary of methods for our benchmark. Notice that the method should distribute the parameters in the tuple `params`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_method(noisy_signal, *args, **kwargs):\n",
    "    # The method should distribute the parameters accordingly, for example as:\n",
    "    output = a_function(noisy_signal, *args, **kwargs)\n",
    "    return output\n",
    "        \n",
    "\n",
    "# Create a dictionary of the methods to test.\n",
    "my_methods = {\n",
    "    'Method 1': another_method, \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this, we can define the different combinations of parameters using the corresponding dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the different combinations of thresholds to test:\n",
    "thr1 = [1.0, 2.0, 3.0] # Positional arguments to try.\n",
    "thr2 = [4, 5, 6] # Some values for the keyword, argument.\n",
    "\n",
    "my_parameters = {\n",
    "    'Method 1': [((t1,),{'thr2': t2,}) for t1 in thr1 for t2 in thr2], # Remember the keys of this dictionary should be same as the methods dictionary.\n",
    "    # 'Method 1': [{'thr1': t1,'thr2': t2,} for t1 in thr1 for t2 in thr2],\n",
    "    # 'Method 1': [(t1,t2,) for t1 in thr1 for t2 in thr2],\n",
    "}\n",
    "\n",
    "print(my_parameters['Method 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have four combinations of input parameters for `another_method()`, that will be passed one by one to the method so that all the experiments will be carried out for each of the combinations. Let us set the benchmark and run a test using this new configuration of methods and parameters. After that, we can use the `Benchmark` class method `get_results_as_df()` to obtain a table with the results as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_p = Benchmark(task = 'denoising',\n",
    "                         methods = my_methods, \n",
    "                         N = 256, \n",
    "                         parameters = my_parameters, \n",
    "                         SNRin = [40, 50],\n",
    "                         using_signals=['LinearChirp', 'CosChirp',], \n",
    "                         repetitions = 3)\n",
    "\n",
    "benchmark_p.run_test() # Run the test. my_results is a dictionary with the results for each of the variables of the simulation.\n",
    "results_parameters = benchmark_p.get_results_as_df() # This formats the results on a DataFrame\n",
    "results_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the experiments have been repeated for every combination of parameters, listed in the second column of the table as `Parameter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating plots with the Results Interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = ResultsInterpreter(benchmark)\n",
    "\n",
    "fig = interpreter.get_summary_plots(savetofile=False)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9d82c022a17ecf4c04a7b7c1a3d83683f3eb170f3c8487f10eb4fe4ba812916"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
