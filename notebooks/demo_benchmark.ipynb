{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A benchmark for time-frequency denoising/ detection methods\n",
    "\n",
    "## What is this benchmark?\n",
    "A benchmark is a comparison between different methods when running an standardized test. The goal of this benchmark is to compare different methods for denoising / detecting a signal based on different characterizations of the of the time-frequency representation of the signal. This particular benchmark has been created for evaluating the performance of techniques based on the zeros of the spectrogram and to contrast them with more traditional methods, like those based on the ridges of that time-frequency distribution.\n",
    "\n",
    "Nevertheless, with the purpose of making this benchmark more useful, the methods to compare, the tests, and the performance evaluation functions were conceived as different modules, so that one can assess new methods without modifing the tests. The only restriction this pose is that the methods should satisfy some requirements regarding the shape of their input an output parameters.  On the one hand, the tests and the performance evaluation functions, are encapsulated in the class `Benchmark`. On the other hand, the signals used in this benchmark are generated by the methods in the class `SignalBank`.\n",
    "\n",
    "In order to compare different methods with possibly different parameters, we need to set up a few things before running the benchmark. A `Benchmark` object receives some input parameters to configure the test:\n",
    "- `task`: This could be `'denoising'` or `'detection'`. The first one compute the quality reconstruction factor (QRF) using the output of the method, whereas the second simply consist in detecting whether a signal is present or not.\n",
    "- `N`: The length of the simulation, i.e. how many samples should the signals have.\n",
    "- `methods`: A dictionary of methods. Each entry of this dictionary corresponds to the function that implements each of the desired methods.\n",
    "- `parameters`: A dictionary of parameters. Each entry of this dictionary corresponds to an array of tuples to pass as input parameters to each method. In order to know which parameters should be passed to each methods, the keys of this dictionary should be the same as those corresponding to the individual methods in the corresponding dictionary. An example of this is showed below.\n",
    "- `SNRin`: A list or tuple of values of SNR to test.\n",
    "- `repetitions`: The number of times the experiment should be repeated with different realizations of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A dummy test \n",
    "First let us define a dummy method for testing. Methods should receive an `M`x`N` numpy array of noisy signals, where `M` is the number of signals, and `N` is the number of their time samples. Additionally, they should receive a second parameter `params` to allow testing different combinations of input parameters. The shape of the output depends on the task (signal denoising or detection). So the signature of a method should be the following:\n",
    "\n",
    " `output = a_method(noisy_signals,params) `.\n",
    "\n",
    "If one set `task='denoising'`, `output` should be also a `M`x`N` numpy array, i.e. the same shape as the input parameter `noisy_signals`, whereas if `task='detection'`, the output should be boolean (`0` or `False` for no signal, and `1` or `True` otherwise).\n",
    "\n",
    "After this, we need to create a *dictionary of methods* to pass the `Benchmark` object at the moment of instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi as pi\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from benchmark_demo.Benchmark import Benchmark\n",
    "from benchmark_demo.ResultsInterpreter import ResultsInterpreter\n",
    "\n",
    "def a_method(noisy_signal, params):\n",
    "    # If additional input parameters are needed, they can be passed in a tuple using `params` and then parsed.\n",
    "    results = noisy_signal # Simply return the same noisy signals.\n",
    "    return results\n",
    "\n",
    "def another_method(noisy_signal, params):\n",
    "    # If additional input parameters are needed, they can be passed in a tuple using `params` and then parsed.\n",
    "    results = 2*noisy_signal # Simply return the same noisy signals.\n",
    "    return results\n",
    "\n",
    "# Create a dictionary of the methods to test.\n",
    "my_methods = {\n",
    "    'Method 1': a_method, \n",
    "    'Method 2': another_method,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `params` in the example above allows us to pass some parameters to our method. This would be useful for testing a single method with several combination of input parameters. In order to do this, we should give the `Benchmark` object a *dictionary of parameters*. An example of this functionality is showed in the next section. For now, lets set the input parameter `parameters = None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to instantiate a `Benchmark` object and run a test using the proposed methods and parameters. The benchmark constructor receives a name of a task (which defines the performance function of the test), a dictionary of the methods to test, the desired length of the signals used in the simulation, a dictionary of different parameters that should be passed to the methods, an array with different values of SNR to test, and the number of repetitions that should be used for each test. Once the object is created, use the class method `run_test()` to start the experiments.\n",
    "\n",
    "*Remark 1: You can use the ```verbosity``` parameter to show less or more messages during the progress of the experiments. There are 4 levels of verbosity, from ```verbosity=0``` (indicate just the start and the end of the experiments) to ```verbostiy = 4``` (show each method and parameter progress)*\n",
    "\n",
    "*Remark 2: Parallelize the experiments is also possible by passing the parameter ```parallelize = True```. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  16\n",
      "Parallel pool: 3\n",
      "Running benchmark...\n",
      "- Signal LinearChirp\n",
      "-- SNR: 40 dB\n"
     ]
    }
   ],
   "source": [
    "benchmark = Benchmark(task = 'denoising',\n",
    "                        methods = my_methods,\n",
    "                        N = 256, \n",
    "                        SNRin = [40, 50], \n",
    "                        repetitions = 3,\n",
    "                        using_signals=['LinearChirp', 'CosChirp',],\n",
    "                        verbosity=4, \n",
    "                        parallelize=3)\n",
    "                        \n",
    "benchmark.run_test() # Run the test. my_results is a dictionary with the results for each of the variables of the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the results of the test in a nested dictionary called `my_results`. In order to get the results in a human-readable way using a `DataFrame`, and also for further analysis and reproducibility, we can use the class method `get_results_as_df()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Signal_id</th>\n",
       "      <th>Repetition</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Method 1</td>\n",
       "      <td>0</td>\n",
       "      <td>CosChirp</td>\n",
       "      <td>0</td>\n",
       "      <td>39.999865</td>\n",
       "      <td>50.000682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Method 1</td>\n",
       "      <td>0</td>\n",
       "      <td>CosChirp</td>\n",
       "      <td>1</td>\n",
       "      <td>39.999735</td>\n",
       "      <td>49.998792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Method 1</td>\n",
       "      <td>0</td>\n",
       "      <td>CosChirp</td>\n",
       "      <td>2</td>\n",
       "      <td>39.999333</td>\n",
       "      <td>49.999958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Method 1</td>\n",
       "      <td>0</td>\n",
       "      <td>LinearChirp</td>\n",
       "      <td>0</td>\n",
       "      <td>40.000330</td>\n",
       "      <td>50.000906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Method 1</td>\n",
       "      <td>0</td>\n",
       "      <td>LinearChirp</td>\n",
       "      <td>1</td>\n",
       "      <td>39.999831</td>\n",
       "      <td>50.001526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Method 1</td>\n",
       "      <td>0</td>\n",
       "      <td>LinearChirp</td>\n",
       "      <td>2</td>\n",
       "      <td>40.001186</td>\n",
       "      <td>50.004968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Method 2</td>\n",
       "      <td>0</td>\n",
       "      <td>CosChirp</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018494</td>\n",
       "      <td>0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Method 2</td>\n",
       "      <td>0</td>\n",
       "      <td>CosChirp</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.007184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Method 2</td>\n",
       "      <td>0</td>\n",
       "      <td>CosChirp</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015696</td>\n",
       "      <td>-0.002415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Method 2</td>\n",
       "      <td>0</td>\n",
       "      <td>LinearChirp</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.016845</td>\n",
       "      <td>-0.005081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Method 2</td>\n",
       "      <td>0</td>\n",
       "      <td>LinearChirp</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.006791</td>\n",
       "      <td>0.001587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Method 2</td>\n",
       "      <td>0</td>\n",
       "      <td>LinearChirp</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>-0.002166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Method Parameter    Signal_id  Repetition         40         50\n",
       "6   Method 1         0     CosChirp           0  39.999865  50.000682\n",
       "7   Method 1         0     CosChirp           1  39.999735  49.998792\n",
       "8   Method 1         0     CosChirp           2  39.999333  49.999958\n",
       "0   Method 1         0  LinearChirp           0  40.000330  50.000906\n",
       "1   Method 1         0  LinearChirp           1  39.999831  50.001526\n",
       "2   Method 1         0  LinearChirp           2  40.001186  50.004968\n",
       "9   Method 2         0     CosChirp           0  -0.018494   0.001025\n",
       "10  Method 2         0     CosChirp           1   0.000485   0.007184\n",
       "11  Method 2         0     CosChirp           2   0.015696  -0.002415\n",
       "3   Method 2         0  LinearChirp           0  -0.016845  -0.005081\n",
       "4   Method 2         0  LinearChirp           1  -0.006791   0.001587\n",
       "5   Method 2         0  LinearChirp           2  -0.013390  -0.002166"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = benchmark.get_results_as_df() # This formats the results on a DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the DataFrame show the results ordered by columns. The first column corresponds to the method identification, and the values are taken from the keys of the dictionary of methods. The second column enumerates the parameters used (more on this on the next section). The third column corresponds to the signal identification, using the signal identification values from the `SignalBank` class. The next column shows the number of repetition of the experiment. Finally, the remaining columns show the results obtained for the SNR values used for each experiment. Since `task = 'denoising'`, these values correspond to the QRF computed as `QRF = 10*np.log10(E(s)/E(s-sr))`, where `E(x)` is the energy of `x`, and `s` and `sr` are the noiseless signal and the reconstructed signal respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing different parameters to the methods.\n",
    "It is common that a method depends on certain input parameters (thresholds, multiplicative factors, etc). Therefore, it would be useful that the tests could also be repeated with different parameters, instead of creating multiple versions of one method. We can pass an array of parameters to a method provided it parses them internally. In order to indicate the benchmark which parameters combinations should be given to each method, a dictionary of parameters can be given. \n",
    "\n",
    "Let us now create this dictionary. The parameters combinations should be given in a tuple of tuples, or a numpy array, so that each internal tuple, or row of the array, is passed as the additional parameter `param` of the method (this latter should implement how to distribute the parameters from this tuple). For this to work, **the keys of this dictionary should be the same as those of the methods dictionary**. \n",
    "\n",
    "We can now see more in detail how to pass different parameters to our methods. For instance, let's consider a function that depends on two thresholds `thr1` and `thr2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_function(signal, thr1, thr2):\n",
    "    signal_out = signal\n",
    "    signal_out[np.where((thr2>signal) & (signal>thr1))] = 1\n",
    "    return signal_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a method that wraps the previous function and then define the dictionary of methods for our benchmark. Notice that the method should distribute the parameters in the tuple `params`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_method(noisy_signals, params):\n",
    "    output_signals = np.zeros(noisy_signals.shape)\n",
    "    # The method should distribute the parameters accordingly, for example as:\n",
    "    thr1 = params[0]\n",
    "    thr2 = params[1]\n",
    "    \n",
    "    output = [a_function(signal, thr1, thr2) for signal in noisy_signals]\n",
    "    output = np.array(output)\n",
    "    return output\n",
    "        \n",
    "\n",
    "# Create a dictionary of the methods to test.\n",
    "my_methods = {\n",
    "    'Method 1': another_method, \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this, we can define the different combinations of parameters using the corresponding dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the different combinations of thresholds to test:\n",
    "thr1 = np.arange(4,6,1)\n",
    "thr2 = [0.1, 0.3, 0.5] # Some values for the thresholds.\n",
    "\n",
    "my_parameters = {\n",
    "    'Method 1': [[t1,t2] for t1 in thr1 for t2 in thr2], # Remember the keys of this dictionary should be same as the methods dictionary.\n",
    "}\n",
    "\n",
    "print((my_parameters['Method 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have four combinations of input parameters for `another_method()`, that will be passed one by one to the method so that all the experiments will be carried out for each of the combinations. Let us set the benchmark and run a test using this new configuration of methods and parameters. After that, we can use the `Benchmark` class method `get_results_as_df()` to obtain a table with the results as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_p = Benchmark(task = 'denoising',\n",
    "                         methods = my_methods, \n",
    "                         N = 256, \n",
    "                         parameters = my_parameters, \n",
    "                         SNRin = [40, 50],\n",
    "                         using_signals=['LinearChirp', 'CosChirp',], \n",
    "                         repetitions = 3)\n",
    "\n",
    "benchmark_p.run_test() # Run the test. my_results is a dictionary with the results for each of the variables of the simulation.\n",
    "results_parameters = benchmark_p.get_results_as_df() # This formats the results on a DataFrame\n",
    "results_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the experiments have been repeated for every combination of parameters, listed in the second column of the table as `Param#`, where `#` is the number corresponding to one group of input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating plots with the Results Interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = ResultsInterpreter(benchmark)\n",
    "\n",
    "fig = interpreter.get_summary_plots(savetofile=False)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a98cd71d001efbb21c27a9287aeec91709bb0600cdd4d40f71e635d8ec818a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
